# Resume Analyser - BERT-based Job Category Classifier

A deep learning model that automatically classifies resumes into 25 job categories using BERT (Bidirectional Encoder Representations from Transformers).

## ğŸ¯ Project Overview

This project implements a state-of-the-art NLP model for resume classification. The model analyzes resume text and predicts the most suitable job category with **100% validation accuracy**.

### Key Features
- âœ… **BERT-base-uncased** fine-tuned for resume classification
- âœ… **25 Job Categories** classification
- âœ… **100% Validation Accuracy** achieved
- âœ… **GPU Accelerated** training on NVIDIA RTX 3060
- âœ… **962 Resume Dataset** from Kaggle

## ğŸ“Š Model Performance

| Metric | Score |
|--------|-------|
| Validation Accuracy | 100% |
| Matthews Correlation Coefficient | 1.0000 |
| Final Training Loss | 0.3604 |
| Final Validation Loss | 0.2767 |

### Training Progress
- **Epoch 1:** 53.37% accuracy â†’ 98.96% accuracy (Epoch 2)
- **Epochs 3-5:** Maintained 100% accuracy

## ğŸ“ Job Categories

The model classifies resumes into 25 categories:
Data Science, Java Developer, Testing, DevOps Engineer, Python Developer, Web Developer, HR, Hadoop, Blockchain, ETL Developer, Operations Manager, Sales, Mechanical Engineer, Arts, Database, Electrical Engineering, Health and Fitness, PMO, Business Analyst, DotNet Developer, Automation Testing, Network Security Engineer, SAP Developer, Civil Engineer, Advocate

## ğŸ› ï¸ Technical Stack

- **Framework:** PyTorch 2.6.0 + CUDA 12.4
- **Transformer Library:** Hugging Face Transformers 4.47.1
- **Model:** BERT-base-uncased (109M parameters)
- **GPU:** NVIDIA GeForce RTX 3060 Laptop GPU (6GB)
- **Language:** Python 3.10

## ğŸ“ Project Structure

```
resume-analyser/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ resume_dataset/          # Kaggle resume dataset
â”œâ”€â”€ models/
â”‚   â””â”€â”€ saved_models/
â”‚       â””â”€â”€ resume_analyser_bert/    # Trained model files
â”‚           â”œâ”€â”€ pytorch_model.bin    # Model weights (436MB)
â”‚           â”œâ”€â”€ config.json          # Model configuration
â”‚           â”œâ”€â”€ tokenizer files      # BERT tokenizer
â”‚           â”œâ”€â”€ training_metrics.json # Training history
â”‚           â””â”€â”€ model_info.txt       # Model details
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ resume_analyser.ipynb        # Training notebook
â”œâ”€â”€ src/
â”‚   â””â”€â”€ check_gpu.py                 # GPU verification script
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (6GB+ VRAM recommended)
- Anaconda/Miniconda

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/resume-analyser.git
cd resume-analyser
```

2. **Create conda environment**
```bash
conda create -n DL-GPU python=3.10
conda activate DL-GPU
```

3. **Install dependencies**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
pip install transformers==4.47.1 tokenizers==0.21.0
pip install pandas scikit-learn matplotlib seaborn kagglehub jupyter
```

4. **Verify GPU setup**
```bash
python src/check_gpu.py
```

### ğŸ“¥ Dataset

The model was trained on the [Resume Dataset](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) from Kaggle (962 resumes, 25 categories).

Download using:
```python
import kagglehub
path = kagglehub.dataset_download("gauravduttakiit/resume-dataset")
```

## ğŸ’» Usage

### Training the Model

Open and run the Jupyter notebook:
```bash
jupyter notebook notebooks/resume_analyser.ipynb
```

The notebook includes:
- Data loading and preprocessing
- Text cleaning and tokenization
- BERT model fine-tuning
- Training with GPU acceleration
- Model evaluation and saving

### Loading the Trained Model

```python
from transformers import BertForSequenceClassification, BertTokenizer
import torch

# Load model and tokenizer
model = BertForSequenceClassification.from_pretrained('models/saved_models/resume_analyser_bert')
tokenizer = BertTokenizer.from_pretrained('models/saved_models/resume_analyser_bert')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.eval()
```

### Making Predictions

```python
# Sample resume text
resume_text = """
Skills: Python, Machine Learning, Deep Learning, PyTorch, TensorFlow
Experience: 3 years in Data Science and AI model development
"""

# Preprocess
inputs = tokenizer(resume_text, 
                   return_tensors='pt', 
                   max_length=200, 
                   padding='max_length', 
                   truncation=True)

# Move to device
inputs = {k: v.to(device) for k, v in inputs.items()}

# Predict
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=1)
    
print(f"Predicted Category ID: {predictions.item()}")
```

## ğŸ”§ Training Configuration

| Parameter | Value |
|-----------|-------|
| Base Model | bert-base-uncased |
| Batch Size | 4 |
| Epochs | 5 |
| Learning Rate | 2e-5 |
| Max Sequence Length | 200 tokens |
| Optimizer | AdamW (foreach=False) |
| Scheduler | Linear warmup |
| Train/Test Split | 80/20 (769/193 samples) |

## ğŸ“ˆ Training Details

- **Training Time:** ~72 minutes (5 epochs)
- **Total Parameters:** 109,501,465 (all trainable)
- **Training Batches:** 193 per epoch
- **Validation Batches:** 49 per epoch
- **GPU Memory:** Optimized for 6GB VRAM

## ğŸ”¬ Model Architecture

```
BertForSequenceClassification
â”œâ”€â”€ BERT Base (12 layers, 768 hidden, 12 attention heads)
â”œâ”€â”€ Dropout (p=0.1)
â””â”€â”€ Linear Classifier (768 â†’ 25 classes)
```

## ğŸ“Š Results & Metrics

The model achieved exceptional performance:
- Perfect classification on validation set (100% accuracy)
- Matthews Correlation Coefficient: 1.0 (perfect correlation)
- Rapid convergence: 98.96% accuracy by Epoch 2
- Consistent performance across all 25 categories

## ğŸ™ Acknowledgments

- [Hugging Face Transformers](https://huggingface.co/transformers/) for the BERT implementation
- [Kaggle Resume Dataset](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) for the training data
- BERT paper: [Devlin et al., 2018](https://arxiv.org/abs/1810.04805)

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

## ğŸ”® Future Improvements

- [ ] Add support for multi-language resumes
- [ ] Implement resume parsing for structured data extraction
- [ ] Create web API for model deployment
- [ ] Add confidence scores and top-k predictions
- [ ] Expand dataset with more resume samples
- [ ] Deploy as a web application

---

**Note:** This model is for educational and research purposes. Ensure compliance with data privacy regulations when using in production.
